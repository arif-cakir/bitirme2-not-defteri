{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## “Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors ##\n",
    "\n",
    "__Intro__\n",
    "\n",
    "uses knn with a compressor based distance metric\n",
    "uses seven in-distributiğon datasets and five out of distriburtion datasets\n",
    "\n",
    "__Related Work__\n",
    "\n",
    "_Compressor based text classification_\n",
    "\n",
    "Used two approaches:\n",
    "1. A compressor to estimate entropy based on Shannon Information Theory\n",
    "2. A compressor to approximate Kolmogorov complexity and information distance\n",
    "\n",
    "First approach mainly employs technique Prediction by Partial Matching. Estimatescross entropy between the probability distribution of a specific class c and a given document d. The point is lower the cross entropy, more likely that d belongs to c.\n",
    "\n",
    "Another method is information distance. It is a distance metric that derived from Kolmogorov complexity. \"For two similar objects, there exists a simple program to convert one to another.\".\n",
    "\n",
    "Authors see that even though previous works explore information distance and classification, none of them combine it with knn merhod. They also use bigger dataset.\n",
    "\n",
    "__Deep Learning for Text Classification__\n",
    "two clusters of deep learning for text classification:\n",
    "transductive learning: represented by Graph Convolutional Networks. (Assumes test dataset is presented during training)\n",
    "Inductive learning: reccurent neural networks and convolutional neural networks. paper is focused on inductive learning.\n",
    "\n",
    "Kolmogorov complexity K(x) characterizes the shortes binary program that can generate x. Basically K(x) gives the lower bound for information. \n",
    "\n",
    "Information distance is defined as $ E(x,y) = $ max{$K(x|y)$, $K(y|x)$} $= K(xy) - $min${$K(x),K(y)$}\n",
    "\n",
    "Kolmogorov complexity is $K(x)$ and Normalized Compression Distance (NCD) uses compression length $C(x)$. NCD can be defined as following:\n",
    "\n",
    "$$ NCD(x,y) = \\frac{C(xy)-min(C(x),C(y))}{max(C(x),C(y))}$$\n",
    "\n",
    "(Generally) higher the compression ratio, closer C(x) to K(x).\n",
    "\n",
    "code doesn't require  any preprocessing or pretraining\n",
    "\n",
    "__Experimental Setup__ \n",
    "\n",
    "Datasets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
